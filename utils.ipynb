{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utils.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/+Hvy69RbWoWyRgvtnaFk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_sHi74wPpcL"
      },
      "source": [
        "# 종목 이름을 입력하면 종목에 해당하는 코드를 불러와 # 네이버 금융(http://finance.naver.com)에 넣어줌 \n",
        "def get_url(item_name, code_df): \n",
        "    code = code_df.query(\"name=='{}'\".format(item_name))['code'].to_string(index=False).lstrip()\n",
        "    url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code) \n",
        "#     url = 'http://finance.naver.com/item/sise.nhn?code={code}'.format(code=code) \n",
        "    print(\"요청 URL = {}\".format(url)) \n",
        "    return url \n",
        "\n",
        "\n",
        "# 신라젠의 일자데이터 url 가져오기 \n",
        "# item_name='신라젠' \n",
        "item_name='GS' \n",
        "url = get_url(item_name, code_df) \n",
        "\n",
        "# 일자 데이터를 담을 df라는 DataFrame 정의 \n",
        "df = pd.DataFrame() \n",
        "\n",
        "# 1페이지에서 20페이지의 데이터만 가져오기 \n",
        "for page in range(1, 21): \n",
        "    pg_url = '{url}&page={page}'.format(url=url, page=page) \n",
        "    df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True) \n",
        "    # df.dropna()를 이용해 결측값 있는 행 제거 \n",
        "    df = df.dropna() \n",
        "    # 상위 5개 데이터 확인하기 \n",
        "    df.head()\n",
        "\n",
        "\n",
        "# 한글로 된 컬럼명을 영어로 바꿔줌 \n",
        "df_eng = df.rename(columns= {'날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open', '고가': 'high', '저가': 'low', '거래량': 'volume'}) \n",
        "\n",
        "df_eng['date'] = pd.to_datetime(df_eng['date'])\n",
        "df_input = df_eng.set_index('date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am2cYIaSPb2f"
      },
      "source": [
        "def calculate_EGR (ds_input,\n",
        "                     idx_datetime,\n",
        "                     unit_period='Week',\n",
        "                     return_span_Y_fitted=False):\n",
        "  ds_sorted_input_with_date_time_index = ds_input.loc[idx_datetime].sort_index()\n",
        "  idx_datetime_sorted = ds_sorted_input_with_date_time_index.index\n",
        "  start_datetime = ds_sorted_input_with_date_time_index.index[0]\n",
        "  end_datetime = ds_sorted_input_with_date_time_index.index[-1]\n",
        "  inferred_freq = pd.infer_freq(ds_sorted_input_with_date_time_index.index)\n",
        "  idx_datetime_range_span = pd.date_range(start = start_datetime, end = end_datetime, freq = inferred_freq)\n",
        "\n",
        "  query_ts = ds_sorted_input_with_date_time_index.values.reshape(-1, 1)\n",
        "\n",
        "  diff_datetime_sec = (end_datetime - start_datetime).total_seconds()\n",
        "\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "\n",
        "  list_available_datetime_idx = [datetime_idx for datetime_idx, datetime in enumerate(idx_datetime_range_span) if datetime in list(idx_datetime_sorted)]\n",
        "  list_span_datetime_idx = [datetime_idx for datetime_idx, _ in enumerate(idx_datetime_range_span)]\n",
        "\n",
        "\n",
        "  # print(\"idx_datetime_range_span \\n\", idx_datetime_range_span)\n",
        "  # print(\"list_available_datetime_idx \\n\", list_available_datetime_idx)\n",
        "  # print(\"list_span_datetime_idx  \\n\", list_span_datetime_idx)\n",
        "\n",
        "  # X_for_fitting = np.array(range(1, len(query_ts)+1)))\n",
        "  X_for_fitting = np.array(list_available_datetime_idx)\n",
        "  Y_for_fitting = query_ts\n",
        "\n",
        "  line_fitter = LinearRegression()\n",
        "  line_fitter.fit(X_for_fitting.reshape(-1, 1), Y_for_fitting)\n",
        "\n",
        "  Y_fitted = line_fitter.predict(X_for_fitting.reshape(-1, 1)).reshape(1, -1)[0]\n",
        "\n",
        "  X_after_filling = np.array(list_span_datetime_idx)\n",
        "  Y_fitted_after_filling = line_fitter.predict(X_after_filling.reshape(-1, 1)).reshape(1, -1)[0]\n",
        "\n",
        "\n",
        "  ds_Y_fitted_without_filling = pd.Series(index = idx_datetime_sorted, data = Y_fitted)\n",
        "  ds_Y_fitted_after_filling = pd.Series(index = idx_datetime_range_span, data = Y_fitted_after_filling)\n",
        "\n",
        "  if return_span_Y_fitted == True:\n",
        "    ds_Y_fitted = ds_Y_fitted_after_filling\n",
        "  else:\n",
        "    ds_Y_fitted = ds_Y_fitted_without_filling \n",
        "\n",
        "\n",
        "  mean_during_window = ds_input.mean()\n",
        "\n",
        "  effective_total_diff = ((line_fitter.coef_[0][0] * len(idx_datetime_range_span)) / mean_during_window)\n",
        "  # print(mean_during_window)\n",
        "\n",
        "  if mean_during_window == 0.0:\n",
        "    effective_growth_rate = 0.0\n",
        "  else:\n",
        "    if unit_period == 'Hour':\n",
        "      effective_growth_rate = (3600)*effective_total_diff / diff_datetime_sec\n",
        "    elif unit_period == 'Day':\n",
        "      effective_growth_rate = (24*3600)*effective_total_diff / diff_datetime_sec\n",
        "    elif unit_period == 'Week':\n",
        "      effective_growth_rate = (7*24*3600)*effective_total_diff / diff_datetime_sec\n",
        "    elif unit_period == 'Month':\n",
        "      effective_growth_rate =  (30*24*3600)*effective_total_diff / diff_datetime_sec\n",
        "    else:\n",
        "      assert unit_period in ['Hour', 'Day', 'Week', 'Month']\n",
        "\n",
        "  # if mean_during_window == 0.0:\n",
        "  #   return 0.0, 0.0, ds_Y_fitted\n",
        "  # else:\n",
        "  #   if unit_period == 'Hour':\n",
        "  #     return effective_total_diff, (3600)*effective_total_diff / diff_datetime_sec, ds_Y_fitted\n",
        "  #   elif unit_period == 'Day':\n",
        "  #     return effective_total_diff, (24*3600)*effective_total_diff / diff_datetime_sec, ds_Y_fitted\n",
        "  #   elif unit_period == 'Week':\n",
        "  #     return effective_total_diff, (7*24*3600)*effective_total_diff / diff_datetime_sec, ds_Y_fitted\n",
        "  #   elif unit_period == 'Month':\n",
        "  #     return effective_total_diff, (30*24*3600)*effective_total_diff / diff_datetime_sec, ds_Y_fitted    \n",
        "  #   else:\n",
        "  #     assert unit_period in ['Hour', 'Day', 'Week', 'Month']\n",
        "\n",
        "  return effective_total_diff, effective_growth_rate, ds_Y_fitted\n",
        "\n",
        "\n",
        "ds_input = df_input['close']\n",
        "idx_datetime = ds_input.index\n",
        "\n",
        "effective_total_diff, effective_growth_rate, ds_Y_fitted = calculate_EGR(ds_input, idx_datetime, unit_period = 'Month')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPAtQ9nyPhlj"
      },
      "source": [
        "from dtw import *\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "dict_reference_trend_shape_input = {\n",
        "    'Increasing': np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
        "    'Decreasing': np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n",
        "}\n",
        "\n",
        "\n",
        "def scale_reference_shape(dict_reference_trend_shape):\n",
        "\n",
        "  list_trend_type = dict_reference_trend_shape.keys()\n",
        "\n",
        "  dict_scaled_reference_trend_shape = {}\n",
        "\n",
        "  for trend_type in list_trend_type:\n",
        "    minMaxScaler = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
        "    reference_ts = dict_reference_trend_shape[trend_type].reshape(-1, 1)\n",
        "    minMaxScaler.fit(reference_ts)\n",
        "    dict_scaled_reference_trend_shape[trend_type] = minMaxScaler.transform(reference_ts)\n",
        "    # dict_reference_trend_shape[trend_type]\n",
        "    # dict_scaled_reference_trend_shape[trend_type] = \n",
        "\n",
        "  return dict_scaled_reference_trend_shape\n",
        "\n",
        "\n",
        "def classify_shape(ds_input,\n",
        "                   idx_datetime,\n",
        "                   dict_reference_trend_shape,\n",
        "                   after_sorting=True,\n",
        "                   using_normalized_distance=True):\n",
        "  if after_sorting == True:\n",
        "    query_ts = ds_input.loc[idx_datetime].sort_index().values.reshape(-1, 1)\n",
        "  else:\n",
        "    query_ts = ds_input.loc[idx_datetime].values.reshape(-1, 1)\n",
        "\n",
        "  from scipy.spatial.distance import euclidean\n",
        "\n",
        "  # from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "  minMaxScaler = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
        "  minMaxScaler.fit(query_ts)\n",
        "  query_ts_scaled = minMaxScaler.transform(query_ts)\n",
        "\n",
        "  if np.var(query_ts_scaled) == 0:\n",
        "    query_ts_scaled[:] = 0.0\n",
        "\n",
        "  list_trend_type = dict_reference_trend_shape.keys()\n",
        "\n",
        "  index_df_distance_result = pd.Index(list_trend_type, name = 'trend type')\n",
        "\n",
        "  df_distance_result = pd.DataFrame(index = index_df_distance_result, columns = ['distance'])\n",
        "\n",
        "  for trend_type in list_trend_type:\n",
        "\n",
        "    reference_ts = dict_reference_trend_shape[trend_type]\n",
        "\n",
        "    tmp_alignment = dtw(query_ts_scaled, reference_ts, keep_internals=True)\n",
        "    \n",
        "    if using_normalized_distance == True:\n",
        "      df_distance_result.loc[trend_type]['distance'] = tmp_alignment.distance/len(query_ts)\n",
        "    else:\n",
        "      df_distance_result.loc[trend_type]['distance'] = tmp_alignment.distance\n",
        "\n",
        "    df_distance_result = df_distance_result.astype('float')\n",
        "\n",
        "  return df_distance_result['distance'].idxmin(), df_distance_result\n",
        "\n",
        "  \n",
        "\n",
        "dict_reference_trend_shape = scale_reference_shape(dict_reference_trend_shape_input)\n",
        "shape_result, df_out = classify_shape(ds_input,\n",
        "                   idx_datetime,\n",
        "                   dict_reference_trend_shape,\n",
        "                   after_sorting=True,\n",
        "                   using_normalized_distance=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}